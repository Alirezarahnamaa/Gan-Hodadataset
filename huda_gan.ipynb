{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "huda gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWxeWu5nc0KY"
      },
      "source": [
        "# ***Imports***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK4j02ILclNE"
      },
      "source": [
        "import os\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras.layers import Input\r\n",
        "from keras.models import Model, Sequential\r\n",
        "from keras.layers.core import Reshape, Dense, Dropout, Flatten\r\n",
        "from keras.layers.advanced_activations import LeakyReLU\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras import initializers\r\n",
        "# Deterministic output.\r\n",
        "# Tired of seeing the same results every time? Remove the line below.\r\n",
        "np.random.seed(1000)\r\n",
        "\r\n",
        "# The results are a little better when the dimensionality of the random vector is only 10.\r\n",
        "# The dimensionality has been left at 784 for consistency with other GAN implementations.\r\n",
        "randomDim = 500\r\n",
        "\r\n",
        "# Optimizer\r\n",
        "adam = Adam(lr=0.0002, beta_1=0.5)\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20XArnDNc281"
      },
      "source": [
        "# ***Load Hoda dataset***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSuelzlHclKg",
        "outputId": "0446d281-8b5e-49a5-a5e1-38f1cedce7a8"
      },
      "source": [
        "from HodaDatasetReader import read_hoda_cdb, read_hoda_dataset\r\n",
        "X_train, Y_train = read_hoda_dataset(dataset_path='/content/Train 60000.cdb',\r\n",
        "                                images_height=28,\r\n",
        "                                images_width=28,\r\n",
        "                                one_hot=False,\r\n",
        "                                reshape=True)\r\n",
        "print(X_train.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyKEkZg6dDgr"
      },
      "source": [
        "# ***Load Mnist dataset***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzPNSp_NuzK_"
      },
      "source": [
        "(X_train, _), (_, _) = mnist.load_data()\r\n",
        "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\r\n",
        "X_train=X_train.reshape(60000, 784)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qWxrPYHdAV2"
      },
      "source": [
        "# ***generator***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ8cph5NclH4"
      },
      "source": [
        "generator = Sequential()\r\n",
        "generator.add(Dense(256, input_dim=randomDim, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\r\n",
        "generator.add(LeakyReLU(0.2))\r\n",
        "generator.add(Dense(512))\r\n",
        "generator.add(LeakyReLU(0.2))\r\n",
        "generator.add(Dense(1024))\r\n",
        "generator.add(LeakyReLU(0.2))\r\n",
        "generator.add(Dense(784, activation='tanh'))\r\n",
        "\r\n",
        "generator.compile(loss='binary_crossentropy', optimizer=adam)\r\n",
        "\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12ssIxcIdEOl"
      },
      "source": [
        "# ***discriminator***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-t3ucQBclFW"
      },
      "source": [
        "\r\n",
        "discriminator = Sequential()\r\n",
        "discriminator.add(Dense(1024, input_dim=784, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\r\n",
        "discriminator.add(LeakyReLU(0.2))\r\n",
        "discriminator.add(Dropout(0.3))\r\n",
        "discriminator.add(Dense(512))\r\n",
        "discriminator.add(LeakyReLU(0.2))\r\n",
        "discriminator.add(Dropout(0.3))\r\n",
        "discriminator.add(Dense(256))\r\n",
        "discriminator.add(LeakyReLU(0.2))\r\n",
        "discriminator.add(Dropout(0.3))\r\n",
        "discriminator.add(Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=adam)\r\n",
        "\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQfefFSAdi5N"
      },
      "source": [
        "# ***Final network***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQlzKXNeclCd"
      },
      "source": [
        "# Combined network\r\n",
        "discriminator.trainable = False\r\n",
        "ganInput = Input(shape=(randomDim,))\r\n",
        "x = generator(ganInput)\r\n",
        "ganOutput = discriminator(x)\r\n",
        "gan = Model(inputs=ganInput, outputs=ganOutput)\r\n",
        "gan.compile(loss='binary_crossentropy', optimizer=adam)\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrgpgCrn8vNC"
      },
      "source": [
        "# ***Save gen_Images***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNKn6Vd4ck7G"
      },
      "source": [
        "\r\n",
        "out_dir = \"./gan\"\r\n",
        "if not os.path.exists(out_dir):\r\n",
        "    os.mkdir(out_dir)\r\n",
        "\r\n",
        "dLosses = []\r\n",
        "gLosses = []\r\n",
        "\r\n",
        "# Plot the loss from each batch\r\n",
        "def plotLoss(epoch):\r\n",
        "    plt.figure(figsize=(10, 8))\r\n",
        "    plt.plot(dLosses, label='Discriminitive loss')\r\n",
        "    plt.plot(gLosses, label='Generative loss')\r\n",
        "    plt.xlabel('Epoch')\r\n",
        "    plt.ylabel('Loss')\r\n",
        "    plt.legend()\r\n",
        "    out = os.path.join(out_dir, 'simple_gan_loss_epoch_%d.png' % epoch)\r\n",
        "    plt.savefig(out)\r\n",
        "\r\n",
        "# Create a wall of generated MNIST images\r\n",
        "def plotGeneratedImages(epoch, examples=100, dim=(10, 10), figsize=(10, 10)):\r\n",
        "    noise = np.random.normal(0, 1, size=[examples, randomDim])\r\n",
        "    generatedImages = generator.predict(noise)\r\n",
        "    generatedImages = generatedImages.reshape(examples, 28, 28)\r\n",
        "\r\n",
        "    plt.figure(figsize=figsize)\r\n",
        "    for i in range(generatedImages.shape[0]):\r\n",
        "        plt.subplot(dim[0], dim[1], i+1)\r\n",
        "        plt.imshow(generatedImages[i], interpolation='nearest', cmap='gray_r')\r\n",
        "        plt.axis('off')\r\n",
        "    plt.tight_layout()\r\n",
        "    out = os.path.join(out_dir, 'simple_gan_generated_image_epoch_%d.png' % epoch)\r\n",
        "    plt.savefig(out)\r\n",
        "\r\n",
        "# Save the generator and discriminator networks (and weights) for later use\r\n",
        "def saveModels(epoch):\r\n",
        "    generator.save('./simple_gan_generator_epoch_%d.h5' % epoch)\r\n",
        "    discriminator.save('./simple_gan_discriminator_epoch_%d.h5' % epoch)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFUzR2zziZmf"
      },
      "source": [
        "# ***train***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92I0vKycyo5P",
        "outputId": "7ea3dfbd-892a-4ec8-b106-f6dac4cfb333"
      },
      "source": [
        "epochs=30\r\n",
        "batchSize=256\r\n",
        "\r\n",
        "batchCount = X_train.shape[0] // batchSize\r\n",
        "print ('Epochs:', epochs)\r\n",
        "print ('Batch size:', batchSize)\r\n",
        "print ('Batches per epoch:', batchCount)\r\n",
        "\r\n",
        "for e in range(1, epochs+1):\r\n",
        "    print ('-'*15, 'Epoch %d' % e, '-'*15)\r\n",
        "    for _ in tqdm(range(batchCount)):\r\n",
        "        # Get a random set of input noise and images\r\n",
        "        noise = np.random.normal(0, 1, size=[batchSize, randomDim])\r\n",
        "        imageBatch = X_train[np.random.randint(0, X_train.shape[0], size=batchSize)]\r\n",
        "\r\n",
        "        # Generate fake Hoda images\r\n",
        "        generatedImages = generator.predict(noise)\r\n",
        "        # print np.shape(imageBatch), np.shape(generatedImages)\r\n",
        "        X = np.concatenate([imageBatch, generatedImages])\r\n",
        "\r\n",
        "        # Labels for generated and real data\r\n",
        "        yDis = np.zeros(2*batchSize)\r\n",
        "        # One-sided label smoothing\r\n",
        "        yDis[:batchSize] = 0.9\r\n",
        "\r\n",
        "        # Train discriminator\r\n",
        "        discriminator.trainable = True\r\n",
        "        dloss = discriminator.train_on_batch(X, yDis)\r\n",
        "\r\n",
        "        # Train generator\r\n",
        "        noise = np.random.normal(0, 1, size=[batchSize, randomDim])\r\n",
        "        yGen = np.ones(batchSize)\r\n",
        "        discriminator.trainable = False\r\n",
        "        gloss = gan.train_on_batch(noise, yGen)\r\n",
        "\r\n",
        "    # Store loss of most recent batch from this epoch\r\n",
        "    dLosses.append(dloss)\r\n",
        "    gLosses.append(gloss)\r\n",
        "\r\n",
        "    if e == 1 or e % 2 == 0:\r\n",
        "        plotGeneratedImages(e)\r\n",
        "        #saveModels(e)\r\n",
        "\r\n",
        "# Plot losses from every epoch\r\n",
        "plotLoss(e)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epochs: 30\n",
            "Batch size: 256\n",
            "Batches per epoch: 234\n",
            "--------------- Epoch 1 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 234/234 [00:17<00:00, 13.35it/s]\n",
            "  1%|          | 2/234 [00:00<00:14, 15.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 2 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 234/234 [00:15<00:00, 15.51it/s]\n",
            "  1%|          | 2/234 [00:00<00:14, 16.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 3 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 234/234 [00:14<00:00, 15.86it/s]\n",
            "  1%|          | 2/234 [00:00<00:14, 16.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 4 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 234/234 [00:14<00:00, 15.89it/s]\n",
            "  1%|          | 2/234 [00:00<00:14, 15.93it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 5 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 234/234 [00:14<00:00, 15.83it/s]\n",
            "  1%|          | 2/234 [00:00<00:14, 15.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 6 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 234/234 [00:15<00:00, 15.57it/s]\n",
            "  1%|          | 2/234 [00:00<00:14, 16.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 7 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 234/234 [00:14<00:00, 15.88it/s]\n",
            "  1%|          | 2/234 [00:00<00:14, 15.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 8 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 234/234 [00:14<00:00, 15.89it/s]\n",
            "  1%|          | 2/234 [00:00<00:14, 16.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 9 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 234/234 [00:14<00:00, 15.96it/s]\n",
            "  1%|          | 2/234 [00:00<00:14, 15.93it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 10 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 234/234 [00:14<00:00, 15.91it/s]\n",
            "  1%|          | 2/234 [00:00<00:14, 15.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 11 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 234/234 [00:14<00:00, 15.92it/s]\n",
            "  1%|          | 2/234 [00:00<00:14, 16.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 12 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 234/234 [00:14<00:00, 15.92it/s]\n",
            "  1%|          | 2/234 [00:00<00:15, 15.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 13 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 234/234 [00:14<00:00, 15.90it/s]\n",
            "  1%|          | 2/234 [00:00<00:14, 16.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 14 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 234/234 [00:15<00:00, 15.25it/s]\n",
            "  1%|          | 2/234 [00:00<00:14, 16.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 15 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 112/234 [00:07<00:08, 15.18it/s]"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}